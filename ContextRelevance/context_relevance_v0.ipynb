{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Context Relevance Score: 4.67\n",
      "Individual Context Relevance:\n",
      "Context 1:\n",
      "Score: 10/10\n",
      "Explanation: The context directly and completely answers the prompt by stating that Paris is the capital of France.\n",
      "\n",
      "Context 2:\n",
      "Score: 3/10\n",
      "Explanation: The context mentions Paris, which is relevant, but does not directly state the capital.\n",
      "\n",
      "Context 3:\n",
      "Score: 1/10\n",
      "Explanation: The context mentions a capital city but is irrelevant to the capital of France.\n",
      "\n",
      "########################################\n",
      "Score: 4.666666666666667\n",
      "Overall Reason:\n",
      "The contexts collectively demonstrate high relevance to the prompt, with an average score of 4.67. Context 1 is particularly relevant as it directly answers the question, while Context 2 offers partial relevance. Context 3 is largely irrelevant. Overall, the contexts provide a strong foundation for answering the prompt.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from litellm import completion\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "SEED = 42\n",
    "TEMPERATURE = 0.0\n",
    "\n",
    "def evaluate_context_relevance(prompt, context):\n",
    "    \"\"\"\n",
    "    Evaluate the relevance of a given context to the prompt.\n",
    "    \n",
    "    Args:\n",
    "    prompt (str): The query or prompt.\n",
    "    context (str): The context to evaluate for relevance.\n",
    "    \n",
    "    Returns:\n",
    "    JSON: A JSON containing relevance score and explanation.\n",
    "    \"\"\"\n",
    "    system_message = \"\"\"\n",
    "    You are an AI assistant tasked with evaluating the relevance of a given context to a specific prompt.\n",
    "    Your goal is to determine how useful the context would be in answering the prompt.\n",
    "    \n",
    "    Please evaluate the relevance on a scale from 0 to 10, where:\n",
    "    0: Completely irrelevant\n",
    "    1-3: Marginally relevant (mentions related concepts but doesn't directly address the prompt)\n",
    "    4-6: Moderately relevant (provides some useful information but doesn't fully answer the prompt)\n",
    "    7-9: Highly relevant (provides most of the information needed to answer the prompt)\n",
    "    10: Perfectly relevant (directly and completely answers the prompt)\n",
    "    \n",
    "    Provide your response in the following format:\n",
    "    Score: [Your score from 0 to 10]\n",
    "    Explanation: [A brief explanation of your scoring, no more than 20 words]\n",
    "    \"\"\"\n",
    "    \n",
    "    user_message = f\"\"\"\n",
    "    Prompt: {prompt}\n",
    "    \n",
    "    Context: {context}\n",
    "    \n",
    "    Please evaluate the relevance of this context to the given prompt.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_tokens=50,\n",
    "        temperature=TEMPERATURE,\n",
    "        seed=SEED\n",
    "    )\n",
    "    \n",
    "    result = response.choices[0].message.content.strip().split(\"\\n\")\n",
    "    score = int(result[0].split(\":\")[1].strip())\n",
    "    explanation = result[1].split(\":\")[1].strip()\n",
    "    \n",
    "    return {\"score\": score, \"explanation\": explanation}\n",
    "\n",
    "\n",
    "\n",
    "def generate_overall_reason(average_score, details):\n",
    "    system_message = \"\"\"\n",
    "    You are an AI assistant tasked with summarizing the relevance of multiple contexts to a specific prompt.\n",
    "    Based on the average relevance score and individual context evaluations, provide an overall explanation\n",
    "    of how relevant the contexts are collectively to answering the prompt.\n",
    "    \n",
    "    Your explanation should be concise (no more than 50 words) and address:\n",
    "    1. The overall relevance of the contexts\n",
    "    2. Any particularly relevant or irrelevant contexts\n",
    "    3. How well the contexts collectively answer the prompt\n",
    "    \"\"\"\n",
    "\n",
    "    user_message = f\"\"\"\n",
    "    Average Relevance Score: {average_score:.2f}\n",
    "    \n",
    "    Individual Context Evaluations:\n",
    "    {', '.join(f\"Context {d['context_index']}: Score {d['score']}/10 - {d['explanation']}\" for d in details)}\n",
    "    \n",
    "    Please provide an overall explanation of the context relevance based on this information.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = completion(\n",
    "        model=\"gpt-4o-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\": user_message}\n",
    "        ],\n",
    "        max_tokens=100,\n",
    "        temperature=TEMPERATURE,\n",
    "        seed=SEED\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "def calculate_context_relevance_score(prompt, contexts):\n",
    "    \"\"\"\n",
    "    Calculate the overall context relevance score for a given prompt and list of contexts.\n",
    "    \n",
    "    Args:\n",
    "    prompt (str): The query or prompt.\n",
    "    contexts (list): A list of context strings.\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (float, list) - The average relevance score (0 to 10) and a list of relevance details.\n",
    "    \"\"\"\n",
    "    if not contexts:\n",
    "        return 0.0, []\n",
    "    \n",
    "    relevance_details = []\n",
    "    total_score = 0\n",
    "    \n",
    "    for i, context in enumerate(contexts):\n",
    "        evaluation = evaluate_context_relevance(prompt, context)\n",
    "        relevance_details.append({\n",
    "            \"context_index\": i+1,\n",
    "            \"score\": evaluation[\"score\"],\n",
    "            \"explanation\": evaluation[\"explanation\"]\n",
    "        })\n",
    "        total_score += evaluation[\"score\"]\n",
    "    \n",
    "    average_score = total_score / len(contexts)\n",
    "\n",
    "    # generate the final reason\n",
    "    overall_reason = generate_overall_reason(average_score=average_score, details=relevance_details)\n",
    "    \n",
    "    return {\n",
    "        'score': average_score, \n",
    "        'details': relevance_details,\n",
    "        'reason': overall_reason\n",
    "    }\n",
    "\n",
    "\n",
    "prompt = \"What is the capital of France?\"\n",
    "contexts = [\n",
    "    \"Paris is the capital and most populous city of France.\",\n",
    "    \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris.\",\n",
    "    \"London is the capital and largest city of England and the United Kingdom.\",\n",
    "]\n",
    "\n",
    "result = calculate_context_relevance_score(prompt, contexts)\n",
    "\n",
    "score = result['score']\n",
    "details = result['details']\n",
    "reason = result['reason']\n",
    "\n",
    "output_string = (\n",
    "    f\"Average Context Relevance Score: {score:.2f}\\n\"\n",
    "    \"Individual Context Relevance:\\n\"\n",
    "    + \"\\n\".join(\n",
    "        f\"Context {detail['context_index']}:\\n\"\n",
    "        f\"Score: {detail['score']}/10\\n\"\n",
    "        f\"Explanation: {detail['explanation']}\\n\"\n",
    "        for detail in details\n",
    "    )\n",
    ")\n",
    "\n",
    "print(output_string)\n",
    "\n",
    "print(\"#\"*40)\n",
    "print(f\"Score: {score}\")\n",
    "print(\"Overall Reason:\")\n",
    "print(reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
